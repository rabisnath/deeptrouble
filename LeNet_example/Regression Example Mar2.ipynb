{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c45060a1",
   "metadata": {},
   "source": [
    "## Setting up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5cd2c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision.transforms import ToTensor\n",
    "#from torchvision.datasets import MNIST\n",
    "from torch.optim import Adam\n",
    "#from torch._C import device\n",
    "#from torch.functional import broadcast_shapes\n",
    "\n",
    "from lenet import Bayesian_LeNet_R, LeNet_R\n",
    "\n",
    "\n",
    "# commented out dependencies will be deleted when this notebook is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb529ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba48d061",
   "metadata": {},
   "source": [
    "## Setting up our datasets and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad624dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "this dataset contains 1000 images of galsim galaxies with the image itself as\n",
    "X with and with the sersic index used to generate each one as Y\n",
    "(the galaxies in the images are comprised of a single sersic profile\n",
    "with 15 arcsec half light radius and varying n)\n",
    "\n",
    "the clean dataset has no noise\n",
    "and the noisy dataset has noise\n",
    "\n",
    "the datasets come in from the .pt files as pytorch Datasets\n",
    "'''\n",
    "clean_galaxies = torch.load('clean_single_component_galaxies_1000_values_of_n.pt')\n",
    "#noisy_galaxies = torch.load('noisy_single_component_galaxies_1000_values_of_n.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a7ac034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_galaxies[0] <- a tuple (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05836dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!! - dataset and model hardcoded here\n",
    "data = clean_galaxies\n",
    "MODEL = Bayesian_LeNet_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fccf4bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 50\n",
    "epochs = 10\n",
    "N = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9bac4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the ratio of train:val:test data\n",
    "train_portion = 0.6\n",
    "val_portion = 0.2\n",
    "test_portion = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48b627e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset in to training, validation and testing\n",
    "n_train = int(train_portion * N)\n",
    "n_val = int(val_portion * N)\n",
    "n_test = int(test_portion * N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06a4c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_train + n_val + n_test != N: print(\"Warning: some datapoints were excluded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f07152a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, val_data, test_data) = random_split(data, [n_train, n_val, n_test], generator=torch.Generator().manual_seed(84))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a88a6185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing dataloaders\n",
    "train_data_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_data_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35ec1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating steps per epoch for training+val set\n",
    "# these numbers are only used for keeping record of the \n",
    "# average training + val loss\n",
    "# they aren't directly used in training\n",
    "training_steps = len(train_data_loader.dataset) // batch_size\n",
    "validation_steps = len(val_data_loader.dataset) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d86b4",
   "metadata": {},
   "source": [
    "## Initializing our model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4538769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MODEL(n_channels=1).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3def576b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracking training history\n",
    "hist = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_acc\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0e43b28e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [20, 1, 5, 5], but got 3-dimensional input of size [50, 495, 495] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nk/5v7kxgsd4jq9wkzh_9_0p6600000gn/T/ipykernel_2192/1083407528.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for i in range(epochs):\\n        # set model to training mode\\n        model.train()\\n        # initializing total training, val loss\\n        total_train_loss = 0\\n        total_val_loss = 0\\n        # initializing total error (in units matching the sersic index)\\n        total_train_error = 0\\n        total_val_error = 0\\n\\n        # training step\\n        for (x, y) in train_data_loader:\\n            # send input to device\\n            (x, y) = (x.to(device), y.to(device))\\n\\n            # perform a forward pass and calc the training loss\\n            prediction = model(x)\\n            loss = loss_fn(prediction, y)\\n            error = abs(prediction - y)\\n\\n            # setting gradients to zero, performing backprop and updating the weights\\n            optimizer.zero_grad()\\n            loss.backward()\\n            optimizer.step()\\n\\n            # tracking training loss and num correct\\n            total_train_loss += loss\\n            total_train_error += error\\n\\n        # validation step\\n        with torch.no_grad(): # turning off autograd for evaluation\\n            # setting model to evaluation mode\\n            model.eval()\\n\\n            for (x, y) in val_data_loader:\\n                # send input to device\\n                (x, y) = (x.to(device), y.to(device))\\n\\n                # perform a forward pass and calc the training loss\\n                prediction = model(x)\\n                loss = loss_fn(prediction, y)\\n                error = abs(prediction - y)\\n                total_val_loss += loss\\n                total_val_error += error\\n\\n        # adding stats to the history object\\n        \\n        avg_train_loss = total_train_loss / training_steps\\n        avg_val_loss = total_val_loss / validation_steps\\n        avg_train_error = total_train_error / len(train_data_loader.dataset)\\n        avg_val_error = total_val_error / len(val_data_loader.dataset)\\n\\n        hist[\\'train_loss\\'].append(avg_train_loss.cpu().detach().numpy())\\n        hist[\\'train_error\\'].append(avg_train_error)\\n        hist[\\'val_loss\\'].append(avg_val_loss.cpu().detach().numpy())\\n        hist[\\'val_error\\'].append(avg_val_error)\\n\\n        # printing\\n        print(\"Epoch {}/{}\".format(i+1, epochs))\\n        print(\"Training Loss: {:.6f}, Training Error: {:.4f}\".format(avg_train_loss, avg_train_error))\\n        print(\"Validation Loss: {:.6f}, Validation Error: {:.4f}\".format(avg_val_loss, avg_val_error))\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2401\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2403\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2404\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis/bayesian_cnns/LeNet_example/lenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;31m# pass the input through our first set of CONV => RELU =>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# POOL layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Thesis/bayesian_cnns/LeNet_example/../LeNet_example/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, sample)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_mu\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mkl_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [20, 1, 5, 5], but got 3-dimensional input of size [50, 495, 495] instead"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "for i in range(epochs):\n",
    "        # set model to training mode\n",
    "        model.train()\n",
    "        # initializing total training, val loss\n",
    "        total_train_loss = 0\n",
    "        total_val_loss = 0\n",
    "        # initializing total error (in units matching the sersic index)\n",
    "        total_train_error = 0\n",
    "        total_val_error = 0\n",
    "\n",
    "        # training step\n",
    "        for (x, y) in train_data_loader:\n",
    "            # send input to device\n",
    "            (x, y) = (x.to(device), y.to(device))\n",
    "\n",
    "            # perform a forward pass and calc the training loss\n",
    "            prediction = model(x)\n",
    "            loss = loss_fn(prediction, y)\n",
    "            error = abs(prediction - y)\n",
    "\n",
    "            # setting gradients to zero, performing backprop and updating the weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # tracking training loss and num correct\n",
    "            total_train_loss += loss\n",
    "            total_train_error += error\n",
    "\n",
    "        # validation step\n",
    "        with torch.no_grad(): # turning off autograd for evaluation\n",
    "            # setting model to evaluation mode\n",
    "            model.eval()\n",
    "\n",
    "            for (x, y) in val_data_loader:\n",
    "                # send input to device\n",
    "                (x, y) = (x.to(device), y.to(device))\n",
    "\n",
    "                # perform a forward pass and calc the training loss\n",
    "                prediction = model(x)\n",
    "                loss = loss_fn(prediction, y)\n",
    "                error = abs(prediction - y)\n",
    "                total_val_loss += loss\n",
    "                total_val_error += error\n",
    "\n",
    "        # adding stats to the history object\n",
    "        \n",
    "        avg_train_loss = total_train_loss / training_steps\n",
    "        avg_val_loss = total_val_loss / validation_steps\n",
    "        avg_train_error = total_train_error / len(train_data_loader.dataset)\n",
    "        avg_val_error = total_val_error / len(val_data_loader.dataset)\n",
    "\n",
    "        hist['train_loss'].append(avg_train_loss.cpu().detach().numpy())\n",
    "        hist['train_error'].append(avg_train_error)\n",
    "        hist['val_loss'].append(avg_val_loss.cpu().detach().numpy())\n",
    "        hist['val_error'].append(avg_val_error)\n",
    "\n",
    "        # printing\n",
    "        print(\"Epoch {}/{}\".format(i+1, epochs))\n",
    "        print(\"Training Loss: {:.6f}, Training Error: {:.4f}\".format(avg_train_loss, avg_train_error))\n",
    "        print(\"Validation Loss: {:.6f}, Validation Error: {:.4f}\".format(avg_val_loss, avg_val_error))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b93ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model, \"Bayesian_LeNet_Regression_Mar2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7a2892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
